{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI_finalTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_YHvLkqjCrc"
      },
      "source": [
        "#IBM 왓슨 인공지능 과정 기말고사\n",
        "### 일자 : 2021년 6월 9일"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb_TX0mojY_5"
      },
      "source": [
        "##이름 : '전수영'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIe4C3EfkTDV"
      },
      "source": [
        "#문제 1 : Gensim을 이용한 Word2Vec(15점)\n",
        "### 다음 문제는 gensim API를 사용하여 Word2Vec으로 word embedding 모델을 구현하는 문제입니다. 여러분이 실습하였던 네이버 영화평 과제와 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypnrK__LrPUd"
      },
      "source": [
        "# imports needed and set up logging\n",
        "import gzip\n",
        "import gensim \n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLkGIxkmTB3R"
      },
      "source": [
        "word embedding 모델을 만들 데이터셋을 다운로드받습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIdqVu1KmRNr",
        "outputId": "820f9126-bf2a-43ee-f46e-411c4cdc78b7"
      },
      "source": [
        "! wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1W-UgYO0FmmnKYTj2oKaGlsfxCcmIU1y9' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1W-UgYO0FmmnKYTj2oKaGlsfxCcmIU1y9\" -O reviews_data.txt.gz && rm -rf /tmp/cookies.txt\n",
        "data_file=\"reviews_data.txt.gz\"\n",
        "\n",
        "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
        "    for i,line in enumerate (f):\n",
        "        print(line)\n",
        "        break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T6Z-46STIQV"
      },
      "source": [
        "다운로드 받은 파일을 한 줄 씩 읽습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVe9Mgutmda-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23f75c0-900a-4a29-8468-b1ab3894fabe"
      },
      "source": [
        "def read_input(input_file):\n",
        "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
        "    \n",
        "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
        "    \n",
        "    with gzip.open (input_file, 'rb') as f:\n",
        "        for i, line in enumerate (f): \n",
        "\n",
        "            if (i%10000==0):\n",
        "                logging.info (\"read {0} reviews\".format (i))\n",
        "            # do some pre-processing and return a list of words for each review text\n",
        "            yield gensim.utils.simple_preprocess (line)\n",
        "\n",
        "# read the tokenized reviews into a list\n",
        "# each review item becomes a serries of words\n",
        "# so this becomes a list of lists\n",
        "documents = list (read_input (data_file))\n",
        "logging.info (\"Done reading data file\")    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-09 05:14:00,454 : INFO : reading file reviews_data.txt.gz...this may take a while\n",
            "2021-06-09 05:14:00,462 : INFO : read 0 reviews\n",
            "2021-06-09 05:14:02,293 : INFO : read 10000 reviews\n",
            "2021-06-09 05:14:04,154 : INFO : read 20000 reviews\n",
            "2021-06-09 05:14:06,337 : INFO : read 30000 reviews\n",
            "2021-06-09 05:14:08,330 : INFO : read 40000 reviews\n",
            "2021-06-09 05:14:10,543 : INFO : read 50000 reviews\n",
            "2021-06-09 05:14:12,640 : INFO : read 60000 reviews\n",
            "2021-06-09 05:14:14,416 : INFO : read 70000 reviews\n",
            "2021-06-09 05:14:16,009 : INFO : read 80000 reviews\n",
            "2021-06-09 05:14:17,731 : INFO : read 90000 reviews\n",
            "2021-06-09 05:14:19,725 : INFO : read 100000 reviews\n",
            "2021-06-09 05:14:21,345 : INFO : read 110000 reviews\n",
            "2021-06-09 05:14:22,987 : INFO : read 120000 reviews\n",
            "2021-06-09 05:14:24,679 : INFO : read 130000 reviews\n",
            "2021-06-09 05:14:26,483 : INFO : read 140000 reviews\n",
            "2021-06-09 05:14:28,133 : INFO : read 150000 reviews\n",
            "2021-06-09 05:14:29,828 : INFO : read 160000 reviews\n",
            "2021-06-09 05:14:31,466 : INFO : read 170000 reviews\n",
            "2021-06-09 05:14:33,217 : INFO : read 180000 reviews\n",
            "2021-06-09 05:14:35,470 : INFO : read 190000 reviews\n",
            "2021-06-09 05:14:37,320 : INFO : read 200000 reviews\n",
            "2021-06-09 05:14:39,111 : INFO : read 210000 reviews\n",
            "2021-06-09 05:14:40,891 : INFO : read 220000 reviews\n",
            "2021-06-09 05:14:42,507 : INFO : read 230000 reviews\n",
            "2021-06-09 05:14:44,211 : INFO : read 240000 reviews\n",
            "2021-06-09 05:14:45,892 : INFO : read 250000 reviews\n",
            "2021-06-09 05:14:46,807 : INFO : Done reading data file\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkoZ1y6eNQlp"
      },
      "source": [
        "##문제1-1. Word2Vec API를 사용하여 다운로드받은 파일에 대한 word embedding 모델을 트레이닝시키십시오.(5점)\n",
        "Word2Vec의 파라미터는 다음과 같이 적용하십시오. \n",
        "size=150, window=10, min_count=2, workers=10, iter=1\n",
        "\n",
        "트레이닝 시키는데 2분 가량 소요됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i55O0qECmiLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17504e5-e244-448b-babd-839c8e45bf43"
      },
      "source": [
        "%time model = gensim.models.word2vec.Word2Vec(documents, size=150, window=10, hs=1, min_count=2, workers=10, iter=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-09 05:14:46,821 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
            "2021-06-09 05:14:46,823 : INFO : collecting all words and their counts\n",
            "2021-06-09 05:14:46,824 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-06-09 05:14:47,090 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
            "2021-06-09 05:14:47,362 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
            "2021-06-09 05:14:47,695 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
            "2021-06-09 05:14:47,994 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
            "2021-06-09 05:14:48,335 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
            "2021-06-09 05:14:48,664 : INFO : PROGRESS: at sentence #60000, processed 11013726 words, keeping 76786 word types\n",
            "2021-06-09 05:14:48,936 : INFO : PROGRESS: at sentence #70000, processed 12637528 words, keeping 83199 word types\n",
            "2021-06-09 05:14:49,197 : INFO : PROGRESS: at sentence #80000, processed 14099754 words, keeping 88459 word types\n",
            "2021-06-09 05:14:49,454 : INFO : PROGRESS: at sentence #90000, processed 15662152 words, keeping 93357 word types\n",
            "2021-06-09 05:14:49,704 : INFO : PROGRESS: at sentence #100000, processed 17164490 words, keeping 97886 word types\n",
            "2021-06-09 05:14:49,960 : INFO : PROGRESS: at sentence #110000, processed 18652295 words, keeping 102132 word types\n",
            "2021-06-09 05:14:50,210 : INFO : PROGRESS: at sentence #120000, processed 20152532 words, keeping 105923 word types\n",
            "2021-06-09 05:14:50,465 : INFO : PROGRESS: at sentence #130000, processed 21684333 words, keeping 110104 word types\n",
            "2021-06-09 05:14:50,742 : INFO : PROGRESS: at sentence #140000, processed 23330209 words, keeping 114108 word types\n",
            "2021-06-09 05:14:50,994 : INFO : PROGRESS: at sentence #150000, processed 24838757 words, keeping 118174 word types\n",
            "2021-06-09 05:14:51,249 : INFO : PROGRESS: at sentence #160000, processed 26390913 words, keeping 118670 word types\n",
            "2021-06-09 05:14:51,515 : INFO : PROGRESS: at sentence #170000, processed 27913919 words, keeping 123356 word types\n",
            "2021-06-09 05:14:51,798 : INFO : PROGRESS: at sentence #180000, processed 29535615 words, keeping 126748 word types\n",
            "2021-06-09 05:14:52,058 : INFO : PROGRESS: at sentence #190000, processed 31096462 words, keeping 129847 word types\n",
            "2021-06-09 05:14:52,336 : INFO : PROGRESS: at sentence #200000, processed 32805274 words, keeping 133255 word types\n",
            "2021-06-09 05:14:52,607 : INFO : PROGRESS: at sentence #210000, processed 34434201 words, keeping 136364 word types\n",
            "2021-06-09 05:14:52,883 : INFO : PROGRESS: at sentence #220000, processed 36083485 words, keeping 139418 word types\n",
            "2021-06-09 05:14:53,134 : INFO : PROGRESS: at sentence #230000, processed 37571765 words, keeping 142399 word types\n",
            "2021-06-09 05:14:53,389 : INFO : PROGRESS: at sentence #240000, processed 39138193 words, keeping 145232 word types\n",
            "2021-06-09 05:14:53,675 : INFO : PROGRESS: at sentence #250000, processed 40695052 words, keeping 147966 word types\n",
            "2021-06-09 05:14:53,826 : INFO : collected 150059 word types from a corpus of 41519358 raw words and 255404 sentences\n",
            "2021-06-09 05:14:53,827 : INFO : Loading a fresh vocabulary\n",
            "2021-06-09 05:14:54,754 : INFO : effective_min_count=2 retains 70537 unique words (47% of original 150059, drops 79522)\n",
            "2021-06-09 05:14:54,756 : INFO : effective_min_count=2 leaves 41439836 word corpus (99% of original 41519358, drops 79522)\n",
            "2021-06-09 05:14:54,972 : INFO : deleting the raw counts dictionary of 150059 items\n",
            "2021-06-09 05:14:54,978 : INFO : sample=0.001 downsamples 55 most-common words\n",
            "2021-06-09 05:14:54,979 : INFO : downsampling leaves estimated 30349251 word corpus (73.2% of prior 41439836)\n",
            "2021-06-09 05:14:55,072 : INFO : constructing a huffman tree from 70537 words\n",
            "2021-06-09 05:14:57,793 : INFO : built huffman tree with maximum node depth 24\n",
            "2021-06-09 05:14:57,963 : INFO : estimated required memory for 70537 words and 150 dimensions: 176342500 bytes\n",
            "2021-06-09 05:14:57,965 : INFO : resetting layer weights\n",
            "2021-06-09 05:15:09,404 : INFO : training model with 10 workers on 70537 vocabulary and 150 features, using sg=0 hs=1 sample=0.001 negative=5 window=10\n",
            "2021-06-09 05:15:10,475 : INFO : EPOCH 1 - PROGRESS: at 0.58% examples, 177371 words/s, in_qsize 20, out_qsize 5\n",
            "2021-06-09 05:15:11,507 : INFO : EPOCH 1 - PROGRESS: at 1.42% examples, 218076 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:15:12,508 : INFO : EPOCH 1 - PROGRESS: at 2.15% examples, 219450 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:13,562 : INFO : EPOCH 1 - PROGRESS: at 2.98% examples, 222777 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:14,643 : INFO : EPOCH 1 - PROGRESS: at 3.71% examples, 219506 words/s, in_qsize 17, out_qsize 2\n",
            "2021-06-09 05:15:15,682 : INFO : EPOCH 1 - PROGRESS: at 4.55% examples, 224625 words/s, in_qsize 17, out_qsize 2\n",
            "2021-06-09 05:15:16,781 : INFO : EPOCH 1 - PROGRESS: at 5.36% examples, 224355 words/s, in_qsize 17, out_qsize 2\n",
            "2021-06-09 05:15:17,827 : INFO : EPOCH 1 - PROGRESS: at 6.16% examples, 226447 words/s, in_qsize 19, out_qsize 4\n",
            "2021-06-09 05:15:18,846 : INFO : EPOCH 1 - PROGRESS: at 6.90% examples, 224754 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:19,914 : INFO : EPOCH 1 - PROGRESS: at 7.68% examples, 226544 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:21,014 : INFO : EPOCH 1 - PROGRESS: at 8.43% examples, 224910 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:22,043 : INFO : EPOCH 1 - PROGRESS: at 9.09% examples, 224787 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:23,056 : INFO : EPOCH 1 - PROGRESS: at 9.77% examples, 226447 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:15:24,067 : INFO : EPOCH 1 - PROGRESS: at 10.35% examples, 226097 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:25,091 : INFO : EPOCH 1 - PROGRESS: at 11.02% examples, 226898 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:26,217 : INFO : EPOCH 1 - PROGRESS: at 11.63% examples, 225448 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:27,242 : INFO : EPOCH 1 - PROGRESS: at 12.23% examples, 226142 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:28,258 : INFO : EPOCH 1 - PROGRESS: at 12.90% examples, 225795 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:15:29,307 : INFO : EPOCH 1 - PROGRESS: at 13.69% examples, 226559 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:30,313 : INFO : EPOCH 1 - PROGRESS: at 14.28% examples, 225966 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:15:31,427 : INFO : EPOCH 1 - PROGRESS: at 15.01% examples, 225990 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:32,434 : INFO : EPOCH 1 - PROGRESS: at 15.76% examples, 226482 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:15:33,450 : INFO : EPOCH 1 - PROGRESS: at 16.44% examples, 227396 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:15:34,547 : INFO : EPOCH 1 - PROGRESS: at 17.08% examples, 226926 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:35,595 : INFO : EPOCH 1 - PROGRESS: at 17.69% examples, 226947 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:36,650 : INFO : EPOCH 1 - PROGRESS: at 18.32% examples, 226321 words/s, in_qsize 20, out_qsize 1\n",
            "2021-06-09 05:15:37,653 : INFO : EPOCH 1 - PROGRESS: at 18.94% examples, 226419 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:38,690 : INFO : EPOCH 1 - PROGRESS: at 19.50% examples, 226213 words/s, in_qsize 16, out_qsize 3\n",
            "2021-06-09 05:15:39,750 : INFO : EPOCH 1 - PROGRESS: at 20.16% examples, 226783 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:40,944 : INFO : EPOCH 1 - PROGRESS: at 20.79% examples, 225980 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:41,956 : INFO : EPOCH 1 - PROGRESS: at 21.75% examples, 226600 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:42,995 : INFO : EPOCH 1 - PROGRESS: at 22.39% examples, 226875 words/s, in_qsize 17, out_qsize 2\n",
            "2021-06-09 05:15:44,074 : INFO : EPOCH 1 - PROGRESS: at 23.07% examples, 227487 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:45,177 : INFO : EPOCH 1 - PROGRESS: at 23.71% examples, 227523 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:15:46,192 : INFO : EPOCH 1 - PROGRESS: at 24.29% examples, 227288 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:47,246 : INFO : EPOCH 1 - PROGRESS: at 25.05% examples, 227240 words/s, in_qsize 13, out_qsize 6\n",
            "2021-06-09 05:15:48,272 : INFO : EPOCH 1 - PROGRESS: at 25.96% examples, 227743 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:49,294 : INFO : EPOCH 1 - PROGRESS: at 26.90% examples, 228263 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:50,318 : INFO : EPOCH 1 - PROGRESS: at 27.87% examples, 228547 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:51,412 : INFO : EPOCH 1 - PROGRESS: at 28.65% examples, 228101 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:52,424 : INFO : EPOCH 1 - PROGRESS: at 29.51% examples, 228240 words/s, in_qsize 19, out_qsize 1\n",
            "2021-06-09 05:15:53,498 : INFO : EPOCH 1 - PROGRESS: at 30.31% examples, 228080 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:54,626 : INFO : EPOCH 1 - PROGRESS: at 31.26% examples, 227695 words/s, in_qsize 20, out_qsize 1\n",
            "2021-06-09 05:15:55,657 : INFO : EPOCH 1 - PROGRESS: at 32.08% examples, 227576 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:15:56,681 : INFO : EPOCH 1 - PROGRESS: at 33.00% examples, 227989 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:57,806 : INFO : EPOCH 1 - PROGRESS: at 33.78% examples, 227911 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:58,813 : INFO : EPOCH 1 - PROGRESS: at 34.58% examples, 227828 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:15:59,846 : INFO : EPOCH 1 - PROGRESS: at 35.42% examples, 228171 words/s, in_qsize 17, out_qsize 2\n",
            "2021-06-09 05:16:00,905 : INFO : EPOCH 1 - PROGRESS: at 36.31% examples, 228102 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:01,989 : INFO : EPOCH 1 - PROGRESS: at 37.12% examples, 228067 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:03,033 : INFO : EPOCH 1 - PROGRESS: at 37.97% examples, 228180 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:04,063 : INFO : EPOCH 1 - PROGRESS: at 38.85% examples, 228099 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:05,063 : INFO : EPOCH 1 - PROGRESS: at 39.75% examples, 228802 words/s, in_qsize 19, out_qsize 1\n",
            "2021-06-09 05:16:06,098 : INFO : EPOCH 1 - PROGRESS: at 40.58% examples, 228696 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:07,201 : INFO : EPOCH 1 - PROGRESS: at 41.50% examples, 228452 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:08,183 : INFO : EPOCH 1 - PROGRESS: at 42.34% examples, 228547 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:09,212 : INFO : EPOCH 1 - PROGRESS: at 43.18% examples, 228597 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:10,214 : INFO : EPOCH 1 - PROGRESS: at 44.06% examples, 228754 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:11,216 : INFO : EPOCH 1 - PROGRESS: at 44.87% examples, 228424 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:12,259 : INFO : EPOCH 1 - PROGRESS: at 45.66% examples, 228184 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:16:13,282 : INFO : EPOCH 1 - PROGRESS: at 46.60% examples, 228825 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:14,286 : INFO : EPOCH 1 - PROGRESS: at 47.23% examples, 228280 words/s, in_qsize 16, out_qsize 3\n",
            "2021-06-09 05:16:15,301 : INFO : EPOCH 1 - PROGRESS: at 48.04% examples, 228372 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:16,305 : INFO : EPOCH 1 - PROGRESS: at 48.89% examples, 228396 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:17,328 : INFO : EPOCH 1 - PROGRESS: at 49.68% examples, 228337 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:18,399 : INFO : EPOCH 1 - PROGRESS: at 50.51% examples, 228235 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:19,417 : INFO : EPOCH 1 - PROGRESS: at 51.32% examples, 228310 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:20,595 : INFO : EPOCH 1 - PROGRESS: at 52.20% examples, 228259 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:21,618 : INFO : EPOCH 1 - PROGRESS: at 52.93% examples, 228319 words/s, in_qsize 20, out_qsize 1\n",
            "2021-06-09 05:16:22,631 : INFO : EPOCH 1 - PROGRESS: at 53.68% examples, 228308 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:23,639 : INFO : EPOCH 1 - PROGRESS: at 54.43% examples, 228246 words/s, in_qsize 16, out_qsize 5\n",
            "2021-06-09 05:16:24,654 : INFO : EPOCH 1 - PROGRESS: at 55.43% examples, 228637 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:25,680 : INFO : EPOCH 1 - PROGRESS: at 56.15% examples, 228290 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:26,802 : INFO : EPOCH 1 - PROGRESS: at 57.09% examples, 228499 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:27,795 : INFO : EPOCH 1 - PROGRESS: at 57.83% examples, 228402 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:28,818 : INFO : EPOCH 1 - PROGRESS: at 58.68% examples, 228559 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:29,902 : INFO : EPOCH 1 - PROGRESS: at 59.47% examples, 228248 words/s, in_qsize 19, out_qsize 1\n",
            "2021-06-09 05:16:30,976 : INFO : EPOCH 1 - PROGRESS: at 60.25% examples, 228098 words/s, in_qsize 17, out_qsize 2\n",
            "2021-06-09 05:16:31,999 : INFO : EPOCH 1 - PROGRESS: at 61.07% examples, 228082 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:16:33,000 : INFO : EPOCH 1 - PROGRESS: at 61.83% examples, 228123 words/s, in_qsize 18, out_qsize 0\n",
            "2021-06-09 05:16:34,067 : INFO : EPOCH 1 - PROGRESS: at 62.76% examples, 228314 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:16:35,117 : INFO : EPOCH 1 - PROGRESS: at 63.70% examples, 228387 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:36,177 : INFO : EPOCH 1 - PROGRESS: at 64.64% examples, 228342 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:37,246 : INFO : EPOCH 1 - PROGRESS: at 65.41% examples, 228401 words/s, in_qsize 20, out_qsize 2\n",
            "2021-06-09 05:16:38,232 : INFO : EPOCH 1 - PROGRESS: at 66.19% examples, 228552 words/s, in_qsize 17, out_qsize 2\n",
            "2021-06-09 05:16:39,335 : INFO : EPOCH 1 - PROGRESS: at 67.11% examples, 228766 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:40,363 : INFO : EPOCH 1 - PROGRESS: at 67.96% examples, 228874 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:41,377 : INFO : EPOCH 1 - PROGRESS: at 68.77% examples, 229007 words/s, in_qsize 16, out_qsize 3\n",
            "2021-06-09 05:16:42,442 : INFO : EPOCH 1 - PROGRESS: at 69.61% examples, 229235 words/s, in_qsize 19, out_qsize 1\n",
            "2021-06-09 05:16:43,464 : INFO : EPOCH 1 - PROGRESS: at 70.32% examples, 229192 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:44,499 : INFO : EPOCH 1 - PROGRESS: at 71.11% examples, 229290 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:45,532 : INFO : EPOCH 1 - PROGRESS: at 71.95% examples, 229455 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:46,535 : INFO : EPOCH 1 - PROGRESS: at 72.79% examples, 229385 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:47,562 : INFO : EPOCH 1 - PROGRESS: at 73.72% examples, 229550 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:48,655 : INFO : EPOCH 1 - PROGRESS: at 74.54% examples, 229560 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:49,745 : INFO : EPOCH 1 - PROGRESS: at 75.35% examples, 229585 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:50,804 : INFO : EPOCH 1 - PROGRESS: at 76.15% examples, 229812 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:51,858 : INFO : EPOCH 1 - PROGRESS: at 76.94% examples, 229889 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:52,915 : INFO : EPOCH 1 - PROGRESS: at 77.62% examples, 229641 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:16:53,922 : INFO : EPOCH 1 - PROGRESS: at 78.41% examples, 229773 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:16:54,931 : INFO : EPOCH 1 - PROGRESS: at 79.19% examples, 229822 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:56,013 : INFO : EPOCH 1 - PROGRESS: at 79.95% examples, 229721 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:16:57,036 : INFO : EPOCH 1 - PROGRESS: at 80.71% examples, 229745 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:58,103 : INFO : EPOCH 1 - PROGRESS: at 81.59% examples, 229938 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:16:59,194 : INFO : EPOCH 1 - PROGRESS: at 82.40% examples, 229817 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:17:00,239 : INFO : EPOCH 1 - PROGRESS: at 83.21% examples, 229799 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:17:01,273 : INFO : EPOCH 1 - PROGRESS: at 83.99% examples, 229860 words/s, in_qsize 20, out_qsize 1\n",
            "2021-06-09 05:17:02,398 : INFO : EPOCH 1 - PROGRESS: at 84.78% examples, 229856 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:17:03,412 : INFO : EPOCH 1 - PROGRESS: at 85.54% examples, 229890 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:17:04,437 : INFO : EPOCH 1 - PROGRESS: at 86.33% examples, 229778 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:17:05,461 : INFO : EPOCH 1 - PROGRESS: at 87.21% examples, 229802 words/s, in_qsize 20, out_qsize 1\n",
            "2021-06-09 05:17:06,588 : INFO : EPOCH 1 - PROGRESS: at 88.16% examples, 229867 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:17:07,635 : INFO : EPOCH 1 - PROGRESS: at 89.02% examples, 229796 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:17:08,641 : INFO : EPOCH 1 - PROGRESS: at 89.82% examples, 229850 words/s, in_qsize 20, out_qsize 1\n",
            "2021-06-09 05:17:09,851 : INFO : EPOCH 1 - PROGRESS: at 90.83% examples, 229883 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:17:10,915 : INFO : EPOCH 1 - PROGRESS: at 91.68% examples, 229892 words/s, in_qsize 20, out_qsize 0\n",
            "2021-06-09 05:17:11,918 : INFO : EPOCH 1 - PROGRESS: at 92.48% examples, 229894 words/s, in_qsize 19, out_qsize 1\n",
            "2021-06-09 05:17:12,922 : INFO : EPOCH 1 - PROGRESS: at 93.22% examples, 229948 words/s, in_qsize 19, out_qsize 1\n",
            "2021-06-09 05:17:13,952 : INFO : EPOCH 1 - PROGRESS: at 94.07% examples, 229962 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:17:14,982 : INFO : EPOCH 1 - PROGRESS: at 94.84% examples, 229862 words/s, in_qsize 17, out_qsize 2\n",
            "2021-06-09 05:17:15,998 : INFO : EPOCH 1 - PROGRESS: at 95.71% examples, 229958 words/s, in_qsize 20, out_qsize 4\n",
            "2021-06-09 05:17:17,007 : INFO : EPOCH 1 - PROGRESS: at 96.55% examples, 230115 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:17:18,021 : INFO : EPOCH 1 - PROGRESS: at 97.37% examples, 230090 words/s, in_qsize 18, out_qsize 1\n",
            "2021-06-09 05:17:19,065 : INFO : EPOCH 1 - PROGRESS: at 98.15% examples, 230064 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:17:20,073 : INFO : EPOCH 1 - PROGRESS: at 99.00% examples, 230117 words/s, in_qsize 19, out_qsize 0\n",
            "2021-06-09 05:17:20,967 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
            "2021-06-09 05:17:21,008 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
            "2021-06-09 05:17:21,080 : INFO : EPOCH 1 - PROGRESS: at 99.83% examples, 230161 words/s, in_qsize 7, out_qsize 1\n",
            "2021-06-09 05:17:21,084 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
            "2021-06-09 05:17:21,093 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
            "2021-06-09 05:17:21,120 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
            "2021-06-09 05:17:21,144 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
            "2021-06-09 05:17:21,157 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2021-06-09 05:17:21,164 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-06-09 05:17:21,165 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-06-09 05:17:21,173 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-06-09 05:17:21,174 : INFO : EPOCH - 1 : training on 41519358 raw words (30351575 effective words) took 131.8s, 230355 effective words/s\n",
            "2021-06-09 05:17:21,177 : INFO : training on a 41519358 raw words (30351575 effective words) took 131.8s, 230336 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 39s, sys: 1.39 s, total: 4min 40s\n",
            "Wall time: 2min 34s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMiqYJmLnsJw"
      },
      "source": [
        "##문제1-2. gensim의 API를 사용하여 'dirty'의 동의어들을 찾아내십시오. \n",
        "아래 링크의 네이버 영화평 코드를 참조하십시오.\n",
        "\n",
        "참조 : https://colab.research.google.com/drive/1kY7mpCXo_5RDB-WpTO_4bARDFaZB_Vyi#scrollTo=45SmN7RK7hJb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lne2vO8mjfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ac47d6-b241-48c9-b99e-71ffd0ea828e"
      },
      "source": [
        "# dirty와 가장 similar한 단어들을 출력하십시오.\n",
        "print(model.wv.most_similar(positive=[\"dirty\"]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-09 05:17:21,192 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('filthy', 0.851281464099884), ('stained', 0.7728515863418579), ('unclean', 0.7650426030158997), ('grubby', 0.7558203935623169), ('smelly', 0.7507476806640625), ('disgusting', 0.7323774695396423), ('dusty', 0.7260020971298218), ('mouldy', 0.7253178358078003), ('dingy', 0.6997331380844116), ('gross', 0.6988166570663452)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV48dmBwQik4"
      },
      "source": [
        "##문제1-3. gensim의 API를 사용하여 'dirty'와 'unclean'간의 코사인 유사도(cosine similarity)를 출력하십시오.\n",
        "다음 링크의 similarity 계산 예제를 참고하십시오.\n",
        "\n",
        "참고 https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U-RaLt7msul",
        "outputId": "0a8a3b06-7569-43a6-ed35-e4533e8276e0"
      },
      "source": [
        "# similarity between 'dirty' and 'unclean'\n",
        "print (model.wv.similarity('dirty', 'unclean'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.76504266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU9JKdNvrP2w"
      },
      "source": [
        "#문제 2: RNN을 이용한 한국어 문장 생성기 코딩(15점)\n",
        "### 다음 코드 중 your_code_here 부분을 완성하십시오. P10-1의 \"글쓰는 인공지능2 실습\"에 사용된 코드를 참고하십시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZDuqyL_rmYG"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkcsdQrnVAY2"
      },
      "source": [
        "학습할 입력 문장을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32jTHO15VAzN"
      },
      "source": [
        "text=\"\"\"특히 최근 수도권에서는 교회 소모임 참석자에 이어 이들 가족과 지인으로 번지는 2차 감염 사례가 증가하고 있습니다. 중앙재난안전대책본부는 오늘까지 수도권 교회와 관련한 코로나19 확진자는 총 63명이라고 밝혔으며  2차 감염자는 33명 이라고 전했습니다. \"\"\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9R3xTkxT1S4"
      },
      "source": [
        "데이터 전처리를 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTQUpKieSpdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57dd309a-d320-4276-f773-78bb792b7dcd"
      },
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts([text])\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)\n",
        "\n",
        "\n",
        "print(t.word_index)\n",
        "\n",
        "sequences = list()\n",
        "for line in text.split('\\n'): \n",
        "    encoded = t.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: %d' % len(sequences))\n",
        "\n",
        "\n",
        "max_len=max(len(l) for l in sequences) \n",
        "\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 31\n",
            "{'2차': 1, '특히': 2, '최근': 3, '수도권에서는': 4, '교회': 5, '소모임': 6, '참석자에': 7, '이어': 8, '이들': 9, '가족과': 10, '지인으로': 11, '번지는': 12, '감염': 13, '사례가': 14, '증가하고': 15, '있습니다': 16, '중앙재난안전대책본부는': 17, '오늘까지': 18, '수도권': 19, '교회와': 20, '관련한': 21, '코로나19': 22, '확진자는': 23, '총': 24, '63명이라고': 25, '밝혔으며': 26, '감염자는': 27, '33명': 28, '이라고': 29, '전했습니다': 30}\n",
            "학습에 사용할 샘플의 개수: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A94dnqA8T8qg"
      },
      "source": [
        "Word Embedding 모델을 Keras로 정의하고 트레이닝을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3xzFR9gTpGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84b05fa-9c4b-4f43-e191-85ea37c39e2e"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_len-1))  \n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=120, verbose=2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "1/1 - 4s - loss: 3.4418 - accuracy: 0.0000e+00\n",
            "Epoch 2/120\n",
            "1/1 - 0s - loss: 3.4226 - accuracy: 0.0333\n",
            "Epoch 3/120\n",
            "1/1 - 0s - loss: 3.4042 - accuracy: 0.1000\n",
            "Epoch 4/120\n",
            "1/1 - 0s - loss: 3.3865 - accuracy: 0.1333\n",
            "Epoch 5/120\n",
            "1/1 - 0s - loss: 3.3693 - accuracy: 0.1333\n",
            "Epoch 6/120\n",
            "1/1 - 0s - loss: 3.3522 - accuracy: 0.1333\n",
            "Epoch 7/120\n",
            "1/1 - 0s - loss: 3.3354 - accuracy: 0.1333\n",
            "Epoch 8/120\n",
            "1/1 - 0s - loss: 3.3188 - accuracy: 0.1333\n",
            "Epoch 9/120\n",
            "1/1 - 0s - loss: 3.3023 - accuracy: 0.1667\n",
            "Epoch 10/120\n",
            "1/1 - 0s - loss: 3.2856 - accuracy: 0.2333\n",
            "Epoch 11/120\n",
            "1/1 - 0s - loss: 3.2682 - accuracy: 0.2333\n",
            "Epoch 12/120\n",
            "1/1 - 0s - loss: 3.2503 - accuracy: 0.2000\n",
            "Epoch 13/120\n",
            "1/1 - 0s - loss: 3.2321 - accuracy: 0.2000\n",
            "Epoch 14/120\n",
            "1/1 - 0s - loss: 3.2138 - accuracy: 0.1667\n",
            "Epoch 15/120\n",
            "1/1 - 0s - loss: 3.1953 - accuracy: 0.2000\n",
            "Epoch 16/120\n",
            "1/1 - 0s - loss: 3.1761 - accuracy: 0.2000\n",
            "Epoch 17/120\n",
            "1/1 - 0s - loss: 3.1561 - accuracy: 0.2333\n",
            "Epoch 18/120\n",
            "1/1 - 0s - loss: 3.1352 - accuracy: 0.2333\n",
            "Epoch 19/120\n",
            "1/1 - 0s - loss: 3.1137 - accuracy: 0.2333\n",
            "Epoch 20/120\n",
            "1/1 - 0s - loss: 3.0916 - accuracy: 0.2333\n",
            "Epoch 21/120\n",
            "1/1 - 0s - loss: 3.0692 - accuracy: 0.2667\n",
            "Epoch 22/120\n",
            "1/1 - 0s - loss: 3.0465 - accuracy: 0.3000\n",
            "Epoch 23/120\n",
            "1/1 - 0s - loss: 3.0234 - accuracy: 0.3000\n",
            "Epoch 24/120\n",
            "1/1 - 0s - loss: 3.0000 - accuracy: 0.3333\n",
            "Epoch 25/120\n",
            "1/1 - 0s - loss: 2.9763 - accuracy: 0.3000\n",
            "Epoch 26/120\n",
            "1/1 - 0s - loss: 2.9521 - accuracy: 0.3000\n",
            "Epoch 27/120\n",
            "1/1 - 0s - loss: 2.9273 - accuracy: 0.3000\n",
            "Epoch 28/120\n",
            "1/1 - 0s - loss: 2.9022 - accuracy: 0.2333\n",
            "Epoch 29/120\n",
            "1/1 - 0s - loss: 2.8771 - accuracy: 0.2667\n",
            "Epoch 30/120\n",
            "1/1 - 0s - loss: 2.8521 - accuracy: 0.2667\n",
            "Epoch 31/120\n",
            "1/1 - 0s - loss: 2.8273 - accuracy: 0.2667\n",
            "Epoch 32/120\n",
            "1/1 - 0s - loss: 2.8028 - accuracy: 0.3000\n",
            "Epoch 33/120\n",
            "1/1 - 0s - loss: 2.7785 - accuracy: 0.3333\n",
            "Epoch 34/120\n",
            "1/1 - 0s - loss: 2.7542 - accuracy: 0.3333\n",
            "Epoch 35/120\n",
            "1/1 - 0s - loss: 2.7301 - accuracy: 0.3333\n",
            "Epoch 36/120\n",
            "1/1 - 0s - loss: 2.7063 - accuracy: 0.3333\n",
            "Epoch 37/120\n",
            "1/1 - 0s - loss: 2.6825 - accuracy: 0.3000\n",
            "Epoch 38/120\n",
            "1/1 - 0s - loss: 2.6590 - accuracy: 0.2667\n",
            "Epoch 39/120\n",
            "1/1 - 0s - loss: 2.6355 - accuracy: 0.2667\n",
            "Epoch 40/120\n",
            "1/1 - 0s - loss: 2.6115 - accuracy: 0.3000\n",
            "Epoch 41/120\n",
            "1/1 - 0s - loss: 2.5872 - accuracy: 0.3333\n",
            "Epoch 42/120\n",
            "1/1 - 0s - loss: 2.5637 - accuracy: 0.3333\n",
            "Epoch 43/120\n",
            "1/1 - 0s - loss: 2.5401 - accuracy: 0.3333\n",
            "Epoch 44/120\n",
            "1/1 - 0s - loss: 2.5170 - accuracy: 0.3667\n",
            "Epoch 45/120\n",
            "1/1 - 0s - loss: 2.4946 - accuracy: 0.3667\n",
            "Epoch 46/120\n",
            "1/1 - 0s - loss: 2.4718 - accuracy: 0.3667\n",
            "Epoch 47/120\n",
            "1/1 - 0s - loss: 2.4496 - accuracy: 0.3667\n",
            "Epoch 48/120\n",
            "1/1 - 0s - loss: 2.4265 - accuracy: 0.3667\n",
            "Epoch 49/120\n",
            "1/1 - 0s - loss: 2.4040 - accuracy: 0.3667\n",
            "Epoch 50/120\n",
            "1/1 - 0s - loss: 2.3813 - accuracy: 0.3667\n",
            "Epoch 51/120\n",
            "1/1 - 0s - loss: 2.3604 - accuracy: 0.3667\n",
            "Epoch 52/120\n",
            "1/1 - 0s - loss: 2.3426 - accuracy: 0.4333\n",
            "Epoch 53/120\n",
            "1/1 - 0s - loss: 2.3247 - accuracy: 0.4333\n",
            "Epoch 54/120\n",
            "1/1 - 0s - loss: 2.3007 - accuracy: 0.4667\n",
            "Epoch 55/120\n",
            "1/1 - 0s - loss: 2.2778 - accuracy: 0.4333\n",
            "Epoch 56/120\n",
            "1/1 - 0s - loss: 2.2632 - accuracy: 0.4333\n",
            "Epoch 57/120\n",
            "1/1 - 0s - loss: 2.2379 - accuracy: 0.4667\n",
            "Epoch 58/120\n",
            "1/1 - 0s - loss: 2.2221 - accuracy: 0.5000\n",
            "Epoch 59/120\n",
            "1/1 - 0s - loss: 2.1998 - accuracy: 0.4667\n",
            "Epoch 60/120\n",
            "1/1 - 0s - loss: 2.1817 - accuracy: 0.5000\n",
            "Epoch 61/120\n",
            "1/1 - 0s - loss: 2.1621 - accuracy: 0.5333\n",
            "Epoch 62/120\n",
            "1/1 - 0s - loss: 2.1424 - accuracy: 0.5000\n",
            "Epoch 63/120\n",
            "1/1 - 0s - loss: 2.1245 - accuracy: 0.6000\n",
            "Epoch 64/120\n",
            "1/1 - 0s - loss: 2.1039 - accuracy: 0.6333\n",
            "Epoch 65/120\n",
            "1/1 - 0s - loss: 2.0861 - accuracy: 0.6000\n",
            "Epoch 66/120\n",
            "1/1 - 0s - loss: 2.0663 - accuracy: 0.6000\n",
            "Epoch 67/120\n",
            "1/1 - 0s - loss: 2.0471 - accuracy: 0.6333\n",
            "Epoch 68/120\n",
            "1/1 - 0s - loss: 2.0291 - accuracy: 0.6000\n",
            "Epoch 69/120\n",
            "1/1 - 0s - loss: 2.0098 - accuracy: 0.6333\n",
            "Epoch 70/120\n",
            "1/1 - 0s - loss: 1.9904 - accuracy: 0.6333\n",
            "Epoch 71/120\n",
            "1/1 - 0s - loss: 1.9732 - accuracy: 0.6667\n",
            "Epoch 72/120\n",
            "1/1 - 0s - loss: 1.9550 - accuracy: 0.7000\n",
            "Epoch 73/120\n",
            "1/1 - 0s - loss: 1.9368 - accuracy: 0.7000\n",
            "Epoch 74/120\n",
            "1/1 - 0s - loss: 1.9187 - accuracy: 0.7000\n",
            "Epoch 75/120\n",
            "1/1 - 0s - loss: 1.9010 - accuracy: 0.7333\n",
            "Epoch 76/120\n",
            "1/1 - 0s - loss: 1.8817 - accuracy: 0.7000\n",
            "Epoch 77/120\n",
            "1/1 - 0s - loss: 1.8635 - accuracy: 0.7000\n",
            "Epoch 78/120\n",
            "1/1 - 0s - loss: 1.8454 - accuracy: 0.7000\n",
            "Epoch 79/120\n",
            "1/1 - 0s - loss: 1.8269 - accuracy: 0.7000\n",
            "Epoch 80/120\n",
            "1/1 - 0s - loss: 1.8092 - accuracy: 0.7000\n",
            "Epoch 81/120\n",
            "1/1 - 0s - loss: 1.7922 - accuracy: 0.7000\n",
            "Epoch 82/120\n",
            "1/1 - 0s - loss: 1.7762 - accuracy: 0.7333\n",
            "Epoch 83/120\n",
            "1/1 - 0s - loss: 1.7608 - accuracy: 0.7333\n",
            "Epoch 84/120\n",
            "1/1 - 0s - loss: 1.7482 - accuracy: 0.7667\n",
            "Epoch 85/120\n",
            "1/1 - 0s - loss: 1.7237 - accuracy: 0.8000\n",
            "Epoch 86/120\n",
            "1/1 - 0s - loss: 1.7071 - accuracy: 0.7667\n",
            "Epoch 87/120\n",
            "1/1 - 0s - loss: 1.6921 - accuracy: 0.7667\n",
            "Epoch 88/120\n",
            "1/1 - 0s - loss: 1.6721 - accuracy: 0.8000\n",
            "Epoch 89/120\n",
            "1/1 - 0s - loss: 1.6584 - accuracy: 0.8333\n",
            "Epoch 90/120\n",
            "1/1 - 0s - loss: 1.6408 - accuracy: 0.7667\n",
            "Epoch 91/120\n",
            "1/1 - 0s - loss: 1.6229 - accuracy: 0.8000\n",
            "Epoch 92/120\n",
            "1/1 - 0s - loss: 1.6078 - accuracy: 0.8333\n",
            "Epoch 93/120\n",
            "1/1 - 0s - loss: 1.5905 - accuracy: 0.8000\n",
            "Epoch 94/120\n",
            "1/1 - 0s - loss: 1.5733 - accuracy: 0.8333\n",
            "Epoch 95/120\n",
            "1/1 - 0s - loss: 1.5571 - accuracy: 0.8333\n",
            "Epoch 96/120\n",
            "1/1 - 0s - loss: 1.5403 - accuracy: 0.8333\n",
            "Epoch 97/120\n",
            "1/1 - 0s - loss: 1.5240 - accuracy: 0.8333\n",
            "Epoch 98/120\n",
            "1/1 - 0s - loss: 1.5073 - accuracy: 0.8333\n",
            "Epoch 99/120\n",
            "1/1 - 0s - loss: 1.4915 - accuracy: 0.8333\n",
            "Epoch 100/120\n",
            "1/1 - 0s - loss: 1.4761 - accuracy: 0.8333\n",
            "Epoch 101/120\n",
            "1/1 - 0s - loss: 1.4583 - accuracy: 0.8333\n",
            "Epoch 102/120\n",
            "1/1 - 0s - loss: 1.4442 - accuracy: 0.8333\n",
            "Epoch 103/120\n",
            "1/1 - 0s - loss: 1.4271 - accuracy: 0.8333\n",
            "Epoch 104/120\n",
            "1/1 - 0s - loss: 1.4122 - accuracy: 0.8333\n",
            "Epoch 105/120\n",
            "1/1 - 0s - loss: 1.3954 - accuracy: 0.8333\n",
            "Epoch 106/120\n",
            "1/1 - 0s - loss: 1.3803 - accuracy: 0.8333\n",
            "Epoch 107/120\n",
            "1/1 - 0s - loss: 1.3650 - accuracy: 0.8333\n",
            "Epoch 108/120\n",
            "1/1 - 0s - loss: 1.3483 - accuracy: 0.8333\n",
            "Epoch 109/120\n",
            "1/1 - 0s - loss: 1.3340 - accuracy: 0.8333\n",
            "Epoch 110/120\n",
            "1/1 - 0s - loss: 1.3177 - accuracy: 0.8333\n",
            "Epoch 111/120\n",
            "1/1 - 0s - loss: 1.3035 - accuracy: 0.8333\n",
            "Epoch 112/120\n",
            "1/1 - 0s - loss: 1.2874 - accuracy: 0.8333\n",
            "Epoch 113/120\n",
            "1/1 - 0s - loss: 1.2723 - accuracy: 0.8333\n",
            "Epoch 114/120\n",
            "1/1 - 0s - loss: 1.2576 - accuracy: 0.8667\n",
            "Epoch 115/120\n",
            "1/1 - 0s - loss: 1.2424 - accuracy: 0.8333\n",
            "Epoch 116/120\n",
            "1/1 - 0s - loss: 1.2281 - accuracy: 0.8667\n",
            "Epoch 117/120\n",
            "1/1 - 0s - loss: 1.2129 - accuracy: 0.9000\n",
            "Epoch 118/120\n",
            "1/1 - 0s - loss: 1.1984 - accuracy: 0.8667\n",
            "Epoch 119/120\n",
            "1/1 - 0s - loss: 1.1841 - accuracy: 0.9333\n",
            "Epoch 120/120\n",
            "1/1 - 0s - loss: 1.1694 - accuracy: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb527bb9e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is_TW5rxUSla"
      },
      "source": [
        "##문제2-1~3. 이 셀에서 3 곳의 your_code_here를 찾아서 코드를 완성하십시오.(15점)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLQpmYxSTqN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669872c9-77a6-402a-b222-6a5cf44b38ce"
      },
      "source": [
        "# 이 블록에서 yoou_code_here 부분을 완성하시오\n",
        "def sentence_generation(model, t, current_word, n): # model = 모델, t = 토크나이저, current_word = 현재 단어, n = 반복할 횟수\n",
        "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
        "    sentence = ''\n",
        "    for _ in range(n): # n번 반복\n",
        "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
        "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # 데이터에 대한 패딩\n",
        "        result = np.argmax(model.predict(encoded), axis=-1)\n",
        "        for word, index in t.word_index.items(): \n",
        "            if index == result: # 만약 예측한 단어의 인덱스 값이 동일한 단어가 있다면\n",
        "                break # 해당 단어가 예측 단어이므로 break\n",
        "        # 아래 부분의 코딩을 완성하십시오        \n",
        "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측한 단어를 current_word로 저장(5점)\n",
        "        sentence = sentence + ' ' + word  # 예측한 단어를 sentence에 append(5점)\n",
        "     \n",
        "    sentence = init_word + sentence # 초기의 텍스트와 for loop에서 생성된 텍스트를 concatenate(5점)\n",
        "    return sentence\n",
        "\n",
        "print(sentence_generation(model, t, '특히', 10))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "특히 최근 최근 최근 최근 최근 이어 이들 가족과 지인으로 가족과\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXAXa6AXrqbU"
      },
      "source": [
        "#문제3 : Keras tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVsWQo8DVNsL"
      },
      "source": [
        "from tensorflow.keras import applications\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLIiyRQ4NJ6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb9e82b-de6d-479d-f8e4-582eb981d217"
      },
      "source": [
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 26.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 31.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 21.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 16.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.8MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c73LL_nXQ2de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22fcabe-7f92-46e6-acc2-92bb7fc7f659"
      },
      "source": [
        "# load CIFAR-10 dataset\n",
        "(img_train, y_train), (img_test, y_test) = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyi1MLXnSg7q"
      },
      "source": [
        "from tensorflow.keras import backend\n",
        "# Normalize pixel values between 0 and 1\n",
        "x_train = img_train.astype('float32') / 255.0\n",
        "x_test = img_test.astype('float32') / 255.0\n",
        "\n",
        "img_rows, img_cols = 32,32\n",
        "if backend.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
        "    input_shape = (img_rows, img_cols, 3)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEqPtul5i5mK"
      },
      "source": [
        "##문제3-1. 전이학습 : VGG16을 pre-trained 모델로  사용할 수 있도록 다음 셀의 코드를 완성하십시오.(5점)\n",
        "아래의 셀에서는 VGG16의 Pre-trained 모델을 사용합니다. 이 방법에 대하여는 P11-2 VGG16실습을 참고하십시오.\n",
        "### 참고 https://keras.io/ko/applications/#vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot6sWWoVK-Nb"
      },
      "source": [
        "# define the model\n",
        "def model_builder(hp):\n",
        "    base_model = applications.VGG16(weights='imagenet', pooling='avg', include_top= False) # your_code_here에 적절한 값을 넣으십시오.\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "    x = base_model(inputs)\n",
        "    for i in range(hp.Int('num_layers', min_value=2, max_value=5, step=1)):\n",
        "        x = Dense(units=hp.Int('units' + str(i), min_value=32, max_value=128, step=32), activation='relu')(x)\n",
        "    x = Dense(10, activation='softmax')(x) \n",
        "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qLUZsnImR8p"
      },
      "source": [
        "##문제3-2. 아래 셀에 tuner 오브젝트 생성하는 코드를 완성하시오. 이 튜너는 RandomSearch를 사용하고 그를 위한 인수값은 다음을 사용하시오.(5점)\n",
        "\n",
        "\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuning_dir',\n",
        "    project_name='ktuner'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_GUsErT17u1"
      },
      "source": [
        "tuner = kt.RandomSearch(model_builder, \n",
        "                        objective='val_accuracy',\n",
        "                        max_trials=3,\n",
        "                        executions_per_trial=1,\n",
        "                        directory='tuning_dir',\n",
        "                        project_name='ktuner')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2g2WQgc5tdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "520b5176-f3dc-4495-c875-9324cde66b61"
      },
      "source": [
        "#Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.\n",
        "%time tuner.search(x_train, y_train, epochs = 1, validation_data = (x_test, y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 819 µs, sys: 0 ns, total: 819 µs\n",
            "Wall time: 932 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlTbmaKYStFL"
      },
      "source": [
        "##문제3-3. 최적의 하이퍼 파라미터들의 조합을 print문으로 출력하시오. (5점)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCjyCGlyOgEe",
        "outputId": "3f870b76-1576-4a5b-b637-12cac254df19"
      },
      "source": [
        "print(tuner.get_best_hyperparameters()[0].values) #아래에 예시한 하이퍼파라미터들을 출력하시오."
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'num_layers': 3, 'units0': 96, 'units1': 128, 'learning_rate': 0.01, 'units2': 32}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2_mC3_7Se80"
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yqKdFCnR_kJ",
        "outputId": "e29bc26c-f355-4dfe-a5fd-8d75e9eca1a2"
      },
      "source": [
        "tuner.get_best_models()[0].summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 512)               14714688  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 96)                49248     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               12416     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 14,780,810\n",
            "Trainable params: 14,780,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNlFFkFOijfn"
      },
      "source": [
        "#코딩 테스트를 완료하였습니다. 1학기-기말고사-yourname.ipynb로 다운로드받아서 LMS와 깃헙에 과제물로 제출해주십시오.\n",
        "#수고 하였습니다!"
      ]
    }
  ]
}